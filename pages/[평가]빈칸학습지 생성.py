import streamlit as st
from kiwipiepy import Kiwi
import docx
from io import BytesIO

# from transformers import BertTokenizer, BertModel
# import torch

# í˜ì´ì§€ ì œëª© ì„¤ì •
st.title("ğŸ§ë¹ˆì¹¸ ëš«ê¸° í•™ìŠµì§€ ë§Œë“¤ê¸°")

# ë‘ ì»¬ëŸ¼ ìƒì„±
col1, col2 = st.columns(2)
with col1:
    st.info('###### ì–¸ì œ ì‚¬ìš©í•˜ë‚˜ìš”?\ní…ìŠ¤íŠ¸ ì…ë ¥í•´ì„œ í•˜ë‚˜í•˜ë‚˜ ì§€ìš°ê³  ë¹ˆì¹¸ì„ ë§Œë“¤ì–´ì•¼ í•  ë•Œ!')
with col2:
    st.warning('###### ì–´ë–»ê²Œ í•´ê²°í•˜ë‚˜ìš”?\nì…ë ¥ë°›ì€ í…ìŠ¤íŠ¸ì˜ í•´ë‹¹ ë¶€ë¶„ì„ í´ë¦­í•˜ë©´ ë¹ˆì¹¸ìœ¼ë¡œ ë§Œë“¤ì–´ì„œ ì›Œë“œíŒŒì¼ë¡œ ìƒì„±í•˜ê¸°')

# ë¹ˆì¹¸ì„ ë§Œë“¤ ë‚´ìš©ì„ ì…ë ¥ë°›ìŒ
contents = st.text_area("ë¹ˆì¹¸ì„ ë§Œë“¤ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”", value="ì˜ì‚¬ëŠ” ì¸ê°„ì˜ ì¡´ì—„ê³¼ ê°€ì¹˜ë¥¼ ì¡´ì¤‘í•˜ë©°, ì˜ë£Œë¥¼ ì ì •í•˜ê³  ê³µì •í•˜ê²Œ ì‹œí–‰í•˜ì—¬ ì¸ë¥˜ì˜ ê±´ê°•ì„ ë³´í˜¸ì¦ì§„í•¨ì— í—Œì‹ í•œë‹¤. ")





tab1, tab2 = st.tabs(['ìˆ˜ì‘ì—…', 'ìë™í™”'])
with tab1:
    # í˜•íƒœì†Œ ë¶„ì„
    kiwi = Kiwi()
    tokens = kiwi.analyze(contents)[0][0]

    # ëª…ì‚¬ë¥¼ ì €ì¥í•  ì§‘í•©
    nouns = set()

    # ê° í† í°ì— ëŒ€í•´ ëª…ì‚¬ ì¶”ì¶œ
    for token in tokens:
        if token.tag == "NNG":
            nouns.add(token.form)

    # ëª…ì‚¬ ì„ íƒ ìœ„ì ¯
    selected_nouns = st.multiselect('ë¹ˆì¹¸ìœ¼ë¡œ ë§Œë“¤ ëª…ì‚¬ë¥¼ ì„ íƒí•˜ì„¸ìš”', list(nouns))

    # 'ìƒì„±í•˜ê¸°' ë²„íŠ¼
    if st.button('ë¹ˆì¹¸ ëš«ê¸° ë¯¸ë¦¬ë³´ê¸°'):
        # ì²´í¬ëœ ëª…ì‚¬ë¥¼ ë¹ˆì¹¸ìœ¼ë¡œ ì¹˜í™˜
        for noun in selected_nouns:
            contents = contents.replace(noun, '___'*len(noun))

        # ê²°ê³¼ í‘œì‹œ
        st.write(contents)

        # ì›Œë“œ ë¬¸ì„œ ìƒì„±
        doc = docx.Document()
        doc.add_paragraph(contents)
        
        # ì›Œë“œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        with BytesIO() as f:
            doc.save(f)
            f.seek(0)
            st.download_button('ì›Œë“œ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ê¸°', f, file_name='í•™ìŠµì§€.docx')

with tab2:
    st.write("ì¤€ë¹„ì¤‘ì…ë‹ˆë‹¤....")
    # # BERT ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
    # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    # model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)

    # # ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
    # text = contents

    # # BERTë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì²˜ë¦¬
    # inputs = tokenizer(text, return_tensors="pt", add_special_tokens=True)
    # outputs = model(**inputs)
    # attentions = outputs.attentions

    # # Attention ê°€ì¤‘ì¹˜ ê³„ì‚° (ë§ˆì§€ë§‰ ë ˆì´ì–´ ì‚¬ìš©)
    # # ì°¨ì› ì¶•ì†Œë¥¼ ìœ„í•´ mean() ëŒ€ì‹  squeeze() ì‚¬ìš©
    # # Attention ê°€ì¤‘ì¹˜ ê³„ì‚° (ë§ˆì§€ë§‰ ë ˆì´ì–´ ì‚¬ìš©)
    # # ì—¬ëŸ¬ ì°¨ì›ì„ ê°€ì§„ í…ì„œë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ mean() ë©”ì†Œë“œ ì‚¬ìš©
    # last_layer_attentions = attentions[-1].squeeze(0)
    # word_attentions = last_layer_attentions.mean(dim=0).squeeze()

    # # ê°€ì¤‘ì¹˜ê°€ ìŠ¤ì¹¼ë¼ê°€ ì•„ë‹ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì¡°ê±´ë¶€ ì²˜ë¦¬
    # weighted_tokens = []
    # for token, weight in zip(tokens, word_attentions):
    #     if weight.numel() == 1:  # numel() ë©”ì†Œë“œëŠ” í…ì„œ ë‚´ ìš”ì†Œì˜ ì´ ê°œìˆ˜ë¥¼ ë°˜í™˜
    #         weighted_tokens.append((token, weight.item()))
    #     else:
    #         # í…ì„œì— ì—¬ëŸ¬ ìš”ì†Œê°€ ìˆëŠ” ê²½ìš° í‰ê· ê°’ ì‚¬ìš©
    #         weighted_tokens.append((token, weight.mean().item()))


    # # ìƒìœ„ 10ê°œ ì¤‘ìš” ë‹¨ì–´ ì¶”ì¶œ
    # top_tokens = sorted(weighted_tokens, key=lambda x: x[1], reverse=True)[:10]

    # # ì¤‘ìš” ë‹¨ì–´ ì„ íƒ ìœ„ì ¯
    # selected_tokens = st.multiselect('ì¤‘ìš” ë‹¨ì–´ ì„ íƒ', [token for token, weight in top_tokens])

    # # 'ìƒì„±í•˜ê¸°' ë²„íŠ¼
    # if st.button('ë¹ˆì¹¸ ìƒì„±í•˜ê¸°'):
    #     # ì„ íƒëœ í† í°ì„ ë¹ˆì¹¸ìœ¼ë¡œ ì¹˜í™˜
    #     for token in selected_tokens:
    #         text = text.replace(token, "____")
    #     st.write(text)